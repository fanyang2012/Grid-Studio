{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Master thesis - Chapter 5: Time series forecasting - Data preparation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPCKRQzKZN7foPNRGprFkB/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fanyang2012/Grid-Studio/blob/master/Master_thesis_Chapter_5_Time_series_forecasting_Data_preparation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhKyYbPsJigL"
      },
      "source": [
        "# 1. Data preprocessing for time series forecasting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZHEqaZbMEL4"
      },
      "source": [
        "## 1.1 Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sYfXV5gJZkY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYCE9drkML5l"
      },
      "source": [
        "## 1.2 Import dataset from clustering results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddVlJmL9L8F4"
      },
      "source": [
        "def clustering(data, n): \n",
        "\n",
        "    num = cluster_num[cluster_num['cluster'] == n]['Skugroupid']\n",
        "    clustering = clustering_result[clustering_result['cluster'] == n].reset_index()\n",
        "    clustering = clustering.drop(columns = ['index'])\n",
        "    for i in range(int(num)): \n",
        "        clusters = clustering['Skugroupid'][i]\n",
        "        data.append(clusters)\n",
        "    print('length of clustering: {} \\n'.format(len(data)))\n",
        "    \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FwYgwECMQnS"
      },
      "source": [
        "def cluster(data,name): \n",
        "\n",
        "    for i in range(len(name)): \n",
        "        clusterid = new_groupdata[new_groupdata['Skugroupid'] == name[i]]\n",
        "        data = data.append(clusterid)\n",
        "    \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhSbxbbTMTD6"
      },
      "source": [
        "new_groupdata = pd.read_csv('new_groupdata')\n",
        "clustering_result = pd.read_csv('clustering_result_4') # 4 clusters from fast moving items\n",
        "cluster5 = pd.read_csv('slow_data') # 1 cluster which belongs to slow moving items\n",
        "cluster_num = clustering_result.groupby('cluster')['Skugroupid'].count().reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMtsrON3MZw2"
      },
      "source": [
        "for i in range(1,5):\n",
        "        #all skugroupids in the cluster\n",
        "        globals()[\"clustering\" + str(i)] = []\n",
        "        globals()[\"clustering\" + str(i)] = clustering(globals()[\"clustering\" + str(i)], i-1)\n",
        "        #data in the cluster\n",
        "        globals()[\"cluster\" + str(i)] = pd.DataFrame(columns=['Skugroupid', 'Day', 'Pick volume'])\n",
        "        globals()[\"cluster\" + str(i)] = cluster(globals()[\"cluster\" + str(i)], globals()[\"clustering\" + str(i)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oDEY0sNM9m2"
      },
      "source": [
        "## 1.3 Select time series from cluster1, cluster3 and cluster5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nD-0tRNTMcVq"
      },
      "source": [
        "ts1 = cluster1[cluster1['Skugroupid'] == 111010]\n",
        "ts1['Day'] = pd.to_datetime(ts1['Day'])\n",
        "ts1.index = np.arange(ts1.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnifRV8YMlGb"
      },
      "source": [
        "sku = cluster3[cluster3['Skugroupid'] == 513030]\n",
        "sku['Day'] = pd.to_datetime(sku['Day'])\n",
        "sku.index = np.arange(sku.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bazWDy4-N38i"
      },
      "source": [
        "slow1 = cluster5[cluster5['Skugroupid'] == 117050]\n",
        "slow1['Day'] = pd.to_datetime(slow1['Day'])\n",
        "slow1.index = np.arange(slow1.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbbx-rkWOCL4"
      },
      "source": [
        "## 1.4 Plot each time series"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOMZzXJJN6ct"
      },
      "source": [
        "def line_chart(data): \n",
        "\n",
        "    plt.figure(figsize=(20,6))\n",
        "    plt.plot(data['Day'], data['Pick volume'], color='blue', marker='.', linestyle='-', linewidth=0.5)\n",
        "    xmin,xmax = plt.xlim()\n",
        "    ymin,ymax = plt.ylim()\n",
        "    plt.grid()\n",
        "    plt.title('Overview of data')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18plDHExOAYH"
      },
      "source": [
        "for data in [slow1, ts1, sku]:\n",
        "    line_chart(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3umSbawQweX"
      },
      "source": [
        "## 1.5 Basic statistical characteristics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga4raLHaOBIf"
      },
      "source": [
        "table = pd.DataFrame({'Time Series':['ts1', 'sku', 'slow1']})\n",
        "table['mean'] = [data['Pick volume'].mean() for data in [ts1, sku, slow1]]\n",
        "table['median'] = [data['Pick volume'].median() for data in [ts1, sku, slow1]]\n",
        "table['std'] = [data['Pick volume'].std() for data in [ts1, sku, slow1]]\n",
        "table['max'] = [data['Pick volume'].max() for data in [ts1, sku, slow1]]\n",
        "table['min'] = [data['Pick volume'].min() for data in [ts1, sku, slow1]]\n",
        "table['sum'] = [data['Pick volume'].sum() for data in [ts1, sku, slow1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WR7TCPo3RPpX"
      },
      "source": [
        "## 1.6 Overview of changes based on each year, month and day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJikU9PJQ2RP"
      },
      "source": [
        "def date_features(data): \n",
        "    \n",
        "    data['Day'] = pd.to_datetime(data['Day'])\n",
        "    \n",
        "    data['Date_Day'] = data['Day'].dt.day\n",
        "    data['Date_weekday'] = data['Day'].dt.weekday\n",
        "    data['Date_week'] = data['Day'].dt.isocalendar().week.astype('int64')\n",
        "    data['Date_month'] = data['Day'].dt.month\n",
        "    data['Date_year'] = data['Day'].dt.year  \n",
        "    \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXnEUHOeQ_9I"
      },
      "source": [
        "for data in [slow1, ts1, sku]:\n",
        "    data = date_features(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSME9dzZRjm_"
      },
      "source": [
        "def plot_change(data, time): \n",
        "\n",
        "    plt.figure(figsize=(20,8))\n",
        "    plt.plot(data[time], data['ts1'], label = 'ts1', color = 'b')\n",
        "    plt.plot(data[time], data['sku'], label = 'sku', color = 'r')\n",
        "    plt.plot(data[time], data['slow1'], label = 'slow1', color = 'g')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z5QKm9aRhGE"
      },
      "source": [
        "### 1.6.1 Overview of changes based on each year"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quxgskLnRah-"
      },
      "source": [
        "trend = pd.DataFrame({'year': [2015, 2016, 2017, 2018, 2019]})\n",
        "trend['ts1'] = ts1.groupby('Date_year')['Pick volume'].mean().tolist()\n",
        "trend['sku'] = sku.groupby('Date_year')['Pick volume'].mean().tolist()\n",
        "trend['slow1'] = slow1.groupby('Date_year')['Pick volume'].mean().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LA1GfLW5Rx4o"
      },
      "source": [
        "# add mean of year in 'table'\n",
        "table['year_mean'] = [trend[col].mean() for col in ['ts1', 'sku', 'slow1']]\n",
        "table['year_std'] = [trend[col].std() for col in ['ts1', 'sku', 'slow1']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b51NaRqVSAYc"
      },
      "source": [
        "plot_change(trend, 'year')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7p2X-7D-RYqJ"
      },
      "source": [
        "### 1.6.2 Overview of changes based on each month"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S19iKnAdSLMV"
      },
      "source": [
        "month = pd.DataFrame({'month': np.arange(1,13)})\n",
        "month['ts1'] = ts1.groupby('Date_month')['Pick volume'].mean().tolist()\n",
        "month['sku'] = sku.groupby('Date_month')['Pick volume'].mean().tolist()\n",
        "month['slow1'] = slow1.groupby('Date_month')['Pick volume'].mean().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTtepydHSLVe"
      },
      "source": [
        "# add mean of month in 'table'\n",
        "table['month_mean'] = [month[col].mean() for col in ['ts1', 'sku', 'slow1']]\n",
        "table['month_std'] = [month[col].std() for col in ['ts1', 'sku', 'slow1']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNzeN_-cSLYP"
      },
      "source": [
        "plot_change(month, 'month')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9MK7EvPSWTf"
      },
      "source": [
        "### 1.6.3 Overview of changes based on each day"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4629Cmo3SLfr"
      },
      "source": [
        "day = pd.DataFrame({'day': np.arange(1,32)})\n",
        "day['ts1'] = ts1.groupby('Date_Day')['Pick volume'].mean().tolist()\n",
        "day['sku'] = sku.groupby('Date_Day')['Pick volume'].mean().tolist()\n",
        "day['slow1'] = slow1.groupby('Date_Day')['Pick volume'].mean().tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QedS7uXgSLkc"
      },
      "source": [
        "# add mean of day in 'table'\n",
        "table['day_mean'] = [day[col].mean() for col in ['ts1', 'sku', 'slow1']]\n",
        "table['day_std'] = [day[col].std() for col in ['ts1', 'sku', 'slow1']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekzlkNkATb_S"
      },
      "source": [
        "# 2. Feature engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfdfLMIudRLm"
      },
      "source": [
        "class CallableBaseTransformerMixin(BaseEstimator, TransformerMixin):\n",
        "  \"\"\"\n",
        "  This basic Mixin allows you to create a stateless transformer from a Callable objects by inheriting from it.\n",
        "  Stateless in this case means that the fit()-step of the transformer is merely a passthrough and\n",
        "  the transformation is independent of the fit()-step.\n",
        "  For instance removing a predetermined string from every column in a pd.DataFrame would be an operation like this,\n",
        "  because it does not inherently require any knowledge from the data that must be acquired in the fit()-step.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def fit(self, X: Iterable, y: Optional[Iterable] = None) -> \"CallableBaseTransformerMixin\":\n",
        "  \"\"\"\n",
        "  The fit function of a Callable transformer is always a passthrough because it is stateless.\n",
        "\n",
        "  Args:\n",
        "  X: data\n",
        "  y: labels\n",
        "\n",
        "  Returns:\n",
        "  returns a pointer to itself\n",
        "\n",
        "  \"\"\"\n",
        "    return self\n",
        "\n",
        "  def transform(self, X: Iterable) -> Iterable:\n",
        "\n",
        "  \"\"\"\n",
        "  The transform() step of a Transformer inheriting from this MixIn will be a simple mapping of the call step.\n",
        "  Essentially __call__() and transform() will do the same thing.\n",
        "\n",
        "  Args:\n",
        "  X: The data to be transformed. This object is simply passed through to the __call__()-function, which must\n",
        "  have a compatible interface of course.\n",
        "\n",
        "\n",
        "  Returns:\n",
        "  The output of the __call__() function.\n",
        "\n",
        "  \"\"\"\n",
        "    return self(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_98-0fsVwM1"
      },
      "source": [
        "import datetime\n",
        "import itertools\n",
        "from copy import deepcopy\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import attr\n",
        "import holidays\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from attr import attrib, attrs\n",
        "\n",
        "GERMAN_STATE_ABBRIVATIONS_MAPPING = {\n",
        "    \"Baden-Württemberg\": \"BW\",\n",
        "    \"Bayern\": \"BY\",\n",
        "    \"Berlin\": \"BE\",\n",
        "    \"Brandenburg\": \"BB\",\n",
        "    \"Bremen\": \"HB\",\n",
        "    \"Hamburg\": \"HH\",\n",
        "    \"Hessen\": \"HE\",\n",
        "    \"Mecklenburg-Vorpommern\": \"MV\",\n",
        "    \"Niedersachsen\": \"NI\",\n",
        "    \"Nordrhein-Westfalen\": \"NW\",\n",
        "    \"Rheinland-Pfalz\": \"RP\",\n",
        "    \"Saarland\": \"SL\",\n",
        "    \"Sachsen\": \"SN\",\n",
        "    \"Sachsen-Anhalt\": \"ST\",\n",
        "    \"Schleswig-Holstein\": \"SH\",\n",
        "    \"Thüringen\": \"TH\",\n",
        "}\n",
        "\n",
        "\n",
        "@attrs(auto_attribs=True, frozen=True, slots=True)\n",
        "class RemoveWeekday(CallableBaseTransformerMixin):\n",
        "    \"\"\"\n",
        "    Removes all rows with a given weekday\n",
        "    Args:\n",
        "        weekday: the weekday (0=Monday, 6=Sunday)\n",
        "        date_column: the name of the date column\n",
        "    \"\"\"\n",
        "\n",
        "    _weekday: int = 5\n",
        "    _date_column: str = \"date\"\n",
        "\n",
        "    def __call__(self, data: pd.DataFrame):\n",
        "        return data[data[self._date_column].dt.weekday != self._weekday]\n",
        "\n",
        "\n",
        "@attrs(auto_attribs=True, frozen=True, slots=True)\n",
        "class GroupedColumnShifter(CallableBaseTransformerMixin):\n",
        "    \"\"\"\n",
        "    This transformer shifts given columns stepwise backwards by within a given range, while the rest of the data is\n",
        "    duplicated for each step. This is performed while the data is grouped by a given list of columns.\n",
        "\n",
        "    Args:\n",
        "        shift_columns:              the set of columns to be shifted\n",
        "        group_columns:              the set of columns to group the data by while performing the shift\n",
        "        shift_identifier_column:    the name of the column that identifies the step size of the shift\n",
        "        shift_range:                the range of steps to be performed\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _shift_columns: Tuple[Tuple[str, int], ...]\n",
        "    _shift_identifier_column: str\n",
        "    _shift_range: Tuple[int, int]\n",
        "    _group_columns: Optional[Tuple[str, ...]] = None\n",
        "\n",
        "    def __call__(self, data: pd.DataFrame):\n",
        "        df_list = [data]\n",
        "        for i in np.arange(self._shift_range[0], self._shift_range[1] + 1, 1):\n",
        "            df = deepcopy(data)\n",
        "            if self._group_columns:\n",
        "                df = df.groupby(list(self._group_columns)).apply(self._shift, i)\n",
        "            else:\n",
        "                df = self._shift(df, i)\n",
        "\n",
        "            df[self._shift_identifier_column] = i\n",
        "            df_list.append(df)\n",
        "        result = pd.concat(df_list)\n",
        "        return result\n",
        "\n",
        "    def _shift(self, data: pd.DataFrame, shift_value: int):\n",
        "        for column, shift_step in self._shift_columns:\n",
        "            if column in data.columns:\n",
        "                i = int(shift_value / shift_step) * shift_step\n",
        "                data[column] = data[column].shift(i)\n",
        "        return data\n",
        "\n",
        "\n",
        "def state_subset(instance, attribute, value):\n",
        "    return set(value).issubset(GERMAN_STATE_ABBRIVATIONS_MAPPING.values())\n",
        "\n",
        "\n",
        "@attrs(auto_attribs=True, frozen=True, slots=True)\n",
        "class UTCTimestampToDate(CallableBaseTransformerMixin):\n",
        "    \"\"\"\n",
        "    Transforms UTCTimestamp to Pandas datetime\n",
        "    Args:\n",
        "        timezone: the timezone of the date\n",
        "        unit: the unit of teh timestamp\n",
        "        column: the name of the timestamp column\n",
        "        name: the name of the new date column\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _timezone: str\n",
        "    _unit: str\n",
        "    _column: str\n",
        "    _name: str\n",
        "\n",
        "    def __call__(self, data: pd.DataFrame):\n",
        "        data[self._name] = pd.to_datetime(data[self._column], unit=self._unit)\n",
        "        data[self._name] = data[self._name].dt.tz_localize(\"UTC\").dt.tz_convert(self._timezone)\n",
        "        data[self._name] = pd.to_datetime(data[self._name].dt.date)\n",
        "        return data\n",
        "\n",
        "\n",
        "@attrs(auto_attribs=True, frozen=True, slots=True)\n",
        "class Substract(CallableBaseTransformerMixin):\n",
        "    \"\"\"\n",
        "    Calculates the difference of th enumeic columns\n",
        "    Args:\n",
        "        column_a: the first column\n",
        "        column_b: the second column\n",
        "        name: the name of the resulting column\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    _column_a: str\n",
        "    _column_b: str\n",
        "    _name: str = \"diff\"\n",
        "\n",
        "    def __call__(self, data: pd.DataFrame):\n",
        "        data[self._name] = data[self._column_a] - data[self._column_b]\n",
        "        return data\n",
        "\n",
        "\n",
        "@attrs(auto_attribs=True, frozen=True, slots=True)\n",
        "class PublicHolidayDeriver(CallableBaseTransformerMixin):\n",
        "    \"\"\"\n",
        "    This deriver returns a boolean feature: True is the given date is a public holiday in any of the given states,\n",
        "    False if not.\n",
        "    Args:\n",
        "        states:         List of states to check for public holidays, in None is given, it will look in all states\n",
        "        date_column:    the name of the date column, needs to be given in datetime\n",
        "        holiday_column: the name of the new feature column\n",
        "        how:            if 'any' (default) it is true if the date is a holiday in any of the states, if 'all' it is\n",
        "                        True if the date is  a holiday in all of the states\n",
        "    \"\"\"\n",
        "\n",
        "    _date_column: str = \"date\"\n",
        "    _holiday_column: str = \"public_holiday\"\n",
        "    _states: Tuple[str, ...] = attrib(default=tuple(GERMAN_STATE_ABBRIVATIONS_MAPPING.values()), validator=state_subset)\n",
        "    _how: str = attrib(default=\"any\", validator=attr.validators.in_([\"any\", \"all\"]))\n",
        "\n",
        "    def __call__(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        start = data[self._date_column].min()\n",
        "        end = data[self._date_column].max()\n",
        "        holidays_de = []\n",
        "        if self._how == \"any\":\n",
        "            for state in self._states:\n",
        "                holidays_state = holidays.Germany(years=list(np.arange(start.year, end.year + 1, 1)), prov=state)\n",
        "                holidays_de.append(holidays_state)\n",
        "            all_holidays = set(itertools.chain.from_iterable(holidays_de))\n",
        "        else:\n",
        "            all_holidays = holidays.Germany(years=list(np.arange(start.year, end.year + 1, 1)), prov=self._states)\n",
        "\n",
        "        data[self._holiday_column] = data[self._date_column].isin(all_holidays)\n",
        "        return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Da4ki8OTeaK"
      },
      "source": [
        "holiday_pipeline_feature = Pipeline(\n",
        "    [(\"timestamp to date\",\n",
        "                UTCTimestampToDate(column='Day', name='Day', unit=\"ms\", timezone=\"Europe/Berlin\"),\n",
        "            ),\n",
        "        (\n",
        "            \"school holiday deriver\",\n",
        "            SchoolHolidayDeriver(\n",
        "                states=None, holiday_column=f\"school_holiday\", date_column='Day', how=\"any\"\n",
        "            ),\n",
        "        ),\n",
        "        (\n",
        "            f\"count till school holiday\",\n",
        "            CountTillEvent(date_column='Day', event_column=f\"school_holiday\", name=f\"countTillSchoolHoliday\"),\n",
        "        ),\n",
        "        (\n",
        "            f\"count since school holiday\",\n",
        "            CountSinceEvent(\n",
        "                date_column='Day', event_column=f\"school_holiday\", name=f\"countSinceSchoolHoliday\"\n",
        "            )),\n",
        "        (\n",
        "            f\"public holiday deriver\",\n",
        "            PublicHolidayDeriver(\n",
        "                date_column='Day',\n",
        "              #  states=None,\n",
        "                holiday_column=f\"public_holiday\",\n",
        "                how=\"all\")),\n",
        "        (\n",
        "            f\"count till public holiday\",\n",
        "            CountTillEvent(\n",
        "                date_column='Day',\n",
        "                event_column=f\"public_holiday\",\n",
        "                name=f\"countTillPublicHoliday\",\n",
        "            )),\n",
        "        (\n",
        "            f\"count since public holiday\",\n",
        "            CountSinceEvent(\n",
        "                date_column='Day',\n",
        "                event_column=f\"public_holiday\",\n",
        "                name=f\"countSincePublicHoliday\",\n",
        "            ))  \n",
        "    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NscX9xHaTjEE"
      },
      "source": [
        "dif_pipeline_feature = Pipeline(\n",
        "        [\n",
        "            (\n",
        "                \"difference month - quarter\",\n",
        "                Substract(\"MeanPickCountLast28Days\", \"MeanPickCountLast84Days\", \"Diff_meanPickCountLast28_84Days\"),\n",
        "            ),\n",
        "            (\n",
        "                \"difference week - month\",\n",
        "                Substract(\"MeanPickCountLast7Days\", \"MeanPickCountLast28Days\", \"Diff_meanPickCountLast7_28Days\"),\n",
        "            ),\n",
        "            (\n",
        "                \"difference weekday - 4weekdays\",\n",
        "                Substract(\"PickCountLastWeekday\", \"MeanPickCount4Weekdays\", \"Diff_meanPickCountLast1_4weekdays\"),\n",
        "            )\n",
        "        ]\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8Y3n7MCTluv"
      },
      "source": [
        "def feature_extraction(data, n=1): \n",
        "    \n",
        "    data['Day'] = pd.to_datetime(data['Day'])\n",
        "\n",
        "    #Add features with date features\n",
        "    data['Date_Day'] = data['Day'].dt.day\n",
        "    data['Date_weekday'] = data['Day'].dt.weekday\n",
        "    data['Date_week'] = data['Day'].dt.isocalendar().week.astype('int64')\n",
        "    data['Date_month'] = data['Day'].dt.month\n",
        "    data['Date_year'] = data['Day'].dt.year\n",
        "    data['Date_is_year_start'] = data['Day'].dt.is_year_start\n",
        "    data['Date_is_year_end'] = data['Day'].dt.is_year_end\n",
        "    data['Date_is_quarter_start'] = data['Day'].dt.is_quarter_start\n",
        "    data['Date_is_quarter_end'] = data['Day'].dt.is_year_end\n",
        "    data['Date_is_month_start'] = data['Day'].dt.is_month_start\n",
        "    data['Date_is_month_end'] = data['Day'].dt.is_month_end    \n",
        "    \n",
        "    #Add holiday features\n",
        "    data['school_holiday'] = np.nan\n",
        "    data['countTillSchoolHoliday'] = np.nan\n",
        "    data['countSinceSchoolHoliday'] = np.nan\n",
        "    data['public_holiday'] = np.nan\n",
        "    data['countTillPublicHoliday'] = np.nan\n",
        "    data['countSincePublicHoliday'] = np.nan\n",
        "\n",
        "    data = holiday_pipeline_feature.fit_transform(data)\n",
        "\n",
        "    #Transfer holiday value and others with boolean of true/false into integer 0/1\n",
        "    data['public_holiday'] = data['public_holiday'].apply(lambda x: int(x == True))\n",
        "    data['school_holiday'] = data['school_holiday'].apply(lambda x: int(x == True))\n",
        "    data['Date_is_year_start'] = data['Date_is_year_start'].apply(lambda x: int(x == True))\n",
        "    data['Date_is_year_end'] = data['Date_is_year_end'].apply(lambda x: int(x == True))\n",
        "    data['Date_is_quarter_start'] = data['Date_is_quarter_start'].apply(lambda x: int(x == True))\n",
        "    data['Date_is_quarter_end'] = data['Date_is_quarter_end'].apply(lambda x: int(x == True))\n",
        "    data['Date_is_month_start'] = data['Date_is_month_start'].apply(lambda x: int(x == True))\n",
        "    data['Date_is_month_end'] = data['Date_is_month_end'].apply(lambda x: int(x == True))  \n",
        "\n",
        "\n",
        "    #Add lag value features with mean / std of pick count in last days\n",
        "    data['MeanPickCountLast7Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=7,min_periods=1).mean().shift(n).bfill())\n",
        "    data['StdPickCountLast7Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=7,min_periods=1).std().shift(n).bfill())\n",
        "    data['MedianPickCountLast7Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=7,min_periods=1).median().shift(n).bfill())\n",
        "    data['MaxPickCountLast7Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=7,min_periods=1).max().shift(n).bfill())\n",
        "    data['MinPickCountLast7Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=7,min_periods=1).min().shift(n).bfill())\n",
        "    data['SumPickCountLast7Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=7,min_periods=1).sum().shift(n).bfill()) \n",
        "    \n",
        "    data['MeanPickCountLast28Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=28,min_periods=1).mean().shift(n).bfill())\n",
        "    data['StdPickCountLast28Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=28,min_periods=1).std().shift(n).bfill())\n",
        "    data['MedianPickCountLast28Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=28,min_periods=1).median().shift(n).bfill())\n",
        "    data['MaxPickCountLast28Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=28,min_periods=1).max().shift(n).bfill())\n",
        "    data['MinPickCountLast28Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=28,min_periods=1).min().shift(n).bfill())\n",
        "    data['SumPickCountLast28Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=28,min_periods=1).sum().shift(n).bfill())     \n",
        "    \n",
        "    data['MeanPickCountLast84Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=84,min_periods=1).mean().shift(n).bfill())\n",
        "    data['StdPickCountLast84Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=84,min_periods=1).std().shift(n).bfill())\n",
        "    data['MedianPickCountLast84Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=84,min_periods=1).median().shift(n).bfill())\n",
        "    data['MaxPickCountLast84Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=84,min_periods=1).max().shift(n).bfill())\n",
        "    data['MinPickCountLast84Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=84,min_periods=1).min().shift(n).bfill())\n",
        "    data['SumPickCountLast84Days'] = data.groupby('Skugroupid')['Pick volume'].apply(lambda x: x.rolling(window=84,min_periods=1).sum().shift(n).bfill()) \n",
        "\n",
        "    #Add lag values features with mean / std of pick count in last weekdays\n",
        "    data['PickCountLastWeekday'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=1, min_periods=1).mean().shift(int(n/7)+1).bfill())\n",
        "    \n",
        "    data['MeanPickCount4Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=4, min_periods=1).mean().shift(int(n/7)+1).bfill())\n",
        "    data['StdPickCount4Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=4, min_periods=1).std().shift(int(n/7)+1).bfill())\n",
        "    data['MedianPickCount4Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=4, min_periods=1).median().shift(int(n/7)+1).bfill())    \n",
        "    data['MaxPickCount4Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=4, min_periods=1).max().shift(int(n/7)+1).bfill())\n",
        "    data['MinPickCount4Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=4, min_periods=1).min().shift(int(n/7)+1).bfill()) \n",
        "    data['SumPickCount4Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=4, min_periods=1).sum().shift(int(n/7)+1).bfill())    \n",
        "    \n",
        "    data['MeanPickCount6Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=6, min_periods=1).mean().shift(int(n/7)+1).bfill())\n",
        "    data['StdPickCount6Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=6, min_periods=1).std().shift(int(n/7)+1).bfill())\n",
        "    data['MedianPickCount6Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=6, min_periods=1).median().shift(int(n/7)+1).bfill())    \n",
        "    data['MaxPickCount6Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=6, min_periods=1).max().shift(int(n/7)+1).bfill())\n",
        "    data['MinPickCount6Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=6, min_periods=1).min().shift(int(n/7)+1).bfill())   \n",
        "    data['SumPickCount6Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=6, min_periods=1).sum().shift(int(n/7)+1).bfill())\n",
        "    \n",
        "    data['MeanPickCount8Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=8, min_periods=1).mean().shift(int(n/7)+1).bfill())\n",
        "    data['StdPickCount8Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=8, min_periods=1).std().shift(int(n/7)+1).bfill())\n",
        "    data['MedianPickCount8Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=8, min_periods=1).median().shift(int(n/7)+1).bfill())    \n",
        "    data['MaxPickCount8Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=8, min_periods=1).max().shift(int(n/7)+1).bfill())\n",
        "    data['MinPickCount8Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=8, min_periods=1).min().shift(int(n/7)+1).bfill())    \n",
        "    data['SumPickCount8Weekdays'] = data.groupby(['Skugroupid', 'Date_weekday'])['Pick volume'].apply(lambda x: x.rolling(window=8, min_periods=1).sum().shift(int(n/7)+1).bfill())\n",
        "    \n",
        "    #Add difference features\n",
        "    data[\"Diff_meanPickCountLast28_84Days\"] = np.nan\n",
        "    data[\"Diff_meanPickCountLast7_28Days\"] = np.nan\n",
        "    data[\"Diff_meanPickCountLast1_4weekdays\"] = np.nan\n",
        "\n",
        "    data = dif_pipeline_feature.fit_transform(data)\n",
        "\n",
        "    #Sort values \n",
        "    data = data.sort_values(['Skugroupid','Day'])\n",
        "\n",
        "    #Convert the string in the column of Skugroupid into integer\n",
        "    data['Skugroupid'] = data['Skugroupid'].astype(int)\n",
        "    \n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "444ORxd1Wqem"
      },
      "source": [
        "# 3. Performance metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmfeKMnJeS1c"
      },
      "source": [
        "def frequency_error(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
        "\"\"\"\n",
        "This metric computes the rate of missed events. An event is defined as a value >= 0.5 a non-event is defined\n",
        "as a value < 0.5. The values of y_pred and y_true are rounded internally, but there will be no side effects, since\n",
        "the function operates on deepcopies exclusively.\n",
        "\n",
        "Args:\n",
        "y_true: array-like of shape = (n_samples) or (n_samples, 1)\n",
        "Ground truth (correct) target values.\n",
        "y_pred : array-like of shape = (n_samples) or (n_samples, 1)\n",
        "Estimated target values.\n",
        "Returns:\n",
        "rate of incorrect identifications of events (y_true >= 1) and non-events (y_true = 0), value is bounded between\n",
        "0 and 1, with 1 meaning all events are identified wrongly, 0.0 the all events are detected correctly and 0.5 meaning a\n",
        "(more or less) random result.\n",
        "\n",
        "\"\"\"\n",
        "# deecopy labels and predictions for rounding\n",
        "  y_true = np.array(y_true)[:]\n",
        "  y_pred = np.array(y_pred)[:]\n",
        "\n",
        "  y_true[y_true >= 0.5] = 1\n",
        "  y_true[y_true < 0.5] = 0\n",
        "  y_pred[y_pred >= 0.5] = 1\n",
        "  y_pred[y_pred < 0.5] = 0\n",
        "\n",
        "  false_pred = sum(y_true != y_pred)\n",
        "  result = false_pred / len(y_true)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNpXd_kLTroC"
      },
      "source": [
        "from typing import Callable, Dict, Optional, Union\n",
        "\n",
        "import numpy as np\n",
        "from numpy import corrcoef\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "def mean_absolute_percentage_error(\n",
        "    y_true: np.ndarray,\n",
        "    y_pred: np.ndarray,\n",
        "    sample_weight: Optional[np.ndarray] = None,\n",
        "    multioutput: Optional[str] = \"uniform_average\",\n",
        ") -> Union[float, np.ndarray]:\n",
        "    epsilon = np.finfo(np.float64).eps\n",
        "    mape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
        "    output_errors = np.average(mape, weights=sample_weight, axis=0)\n",
        "    if isinstance(multioutput, str):\n",
        "        if multioutput == \"raw_values\":\n",
        "            return output_errors\n",
        "        elif multioutput == \"uniform_average\":\n",
        "            # pass None as weights to np.average: uniform mean\n",
        "            multioutput = None\n",
        "\n",
        "    return np.average(output_errors, weights=multioutput)\n",
        "\n",
        "\n",
        "def median_absolute_percentage_error(y_true: np.ndarray, y_pred: np.ndarray, epsilon: Optional[float] = None) -> float:\n",
        "    if epsilon is None:\n",
        "        epsilon = np.finfo(np.float64).eps\n",
        "    ape = np.abs(y_pred - y_true) / np.maximum(np.abs(y_true), epsilon)\n",
        "    mdape = np.median(ape)\n",
        "    return mdape\n",
        "\n",
        "\n",
        "def median_absolute_percentage_error_shifted(\n",
        "    y_true: np.ndarray, y_pred: np.ndarray, add_value: Optional[float] = None\n",
        ") -> float:\n",
        "    if add_value is None:\n",
        "        add_value = 1\n",
        "    y_true = y_true + add_value\n",
        "    y_pred = y_pred + add_value\n",
        "    return median_absolute_percentage_error(y_true, y_pred)\n",
        "\n",
        "\n",
        "def mean_absolute_percentage_error_shifted(\n",
        "    y_true: np.ndarray, y_pred: np.ndarray, add_value: Optional[float] = None\n",
        ") -> float:\n",
        "    if add_value is None:\n",
        "        add_value = 1\n",
        "    y_true = y_true + add_value\n",
        "    y_pred = y_pred + add_value\n",
        "    return float(mean_absolute_percentage_error(y_true, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFfQqXDaXIob"
      },
      "source": [
        "import sklearn.metrics as metrics\n",
        "\n",
        "mean_absolute_percentage_error_shifted(result['y_true'], result['y_pred']) # MAPE\n",
        "median_absolute_percentage_error_shifted(result['y_true'], result['y_pred']) # MdAPE\n",
        "metrics.r2_score(result['y_true'], result['y_pred']) # R squared\n",
        "metrics.mean_absolute_error(result['y_true'], result['y_pred']) # MAE\n",
        "metrics.mean_squared_error(result['y_true'], result['y_pred']) # MSE"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}